{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-3-a8e08a191bd1>, line 101)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-a8e08a191bd1>\"\u001b[0;36m, line \u001b[0;32m101\u001b[0m\n\u001b[0;31m    return resnet_v1(inputs, blocks, num_classes, is_training,\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from nets import resnet_utils\n",
    "\n",
    "\n",
    "resnet_arg_scope = resnet_utils.resnet_arg_scope\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "@slim.add_arg_scope\n",
    "def bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n",
    "               outputs_collections=None, scope=None):\n",
    "  \n",
    "  with tf.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:\n",
    "    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
    "    if depth == depth_in:\n",
    "      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')\n",
    "    else:\n",
    "      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n",
    "                             activation_fn=None, scope='shortcut')\n",
    "\n",
    "    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n",
    "                           scope='conv1')\n",
    "    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n",
    "                                        rate=rate, scope='conv2')\n",
    "    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n",
    "                           activation_fn=None, scope='conv3')\n",
    "\n",
    "    output = tf.nn.relu(shortcut + residual)\n",
    "\n",
    "    return slim.utils.collect_named_outputs(outputs_collections,\n",
    "                                            sc.original_name_scope,\n",
    "                                            output)\n",
    "\n",
    "\n",
    "def resnet_v1(inputs,\n",
    "              blocks,\n",
    "              num_classes=None,\n",
    "              is_training=True,\n",
    "              global_pool=True,\n",
    "              output_stride=None,\n",
    "              include_root_block=True,\n",
    "              spatial_squeeze=True,\n",
    "              reuse=None,\n",
    "              scope=None):\n",
    "  with tf.variable_scope(scope, 'resnet_v1', [inputs], reuse=reuse) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    with slim.arg_scope([slim.conv2d, bottleneck,\n",
    "                         resnet_utils.stack_blocks_dense],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
    "        net = inputs\n",
    "        if include_root_block:\n",
    "          if output_stride is not None:\n",
    "            if output_stride % 4 != 0:\n",
    "              raise ValueError('The output_stride needs to be a multiple of 4.')\n",
    "            output_stride /= 4\n",
    "          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope='conv1')\n",
    "          net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1')\n",
    "        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n",
    "        if global_pool:\n",
    "          # Global average pooling.\n",
    "          net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)\n",
    "        if num_classes is not None:\n",
    "          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
    "                            normalizer_fn=None, scope='logits')\n",
    "        if spatial_squeeze:\n",
    "          logits = tf.squeeze(net, [1, 2], name='SpatialSqueeze')\n",
    "        else:\n",
    "          logits = net\n",
    "        # Convert end_points_collection into a dictionary of end_points.\n",
    "        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "        if num_classes is not None:\n",
    "          end_points['predictions'] = slim.softmax(logits, scope='predictions')\n",
    "        return logits, end_points\n",
    "resnet_v1.default_image_size = 224\n",
    "\n",
    "\n",
    "def resnet_v1_50(inputs,\n",
    "                 num_classes=None,\n",
    "                 is_training=True,\n",
    "                 global_pool=True,\n",
    "                 output_stride=None,\n",
    "                 spatial_squeeze=True,\n",
    "                 reuse=None,\n",
    "                 scope='resnet_v1_50'):\n",
    "    blocks = [\n",
    "      resnet_utils.Block(\n",
    "          'block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
    "      resnet_utils.Block(\n",
    "          'block2', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n",
    "      resnet_utils.Block(\n",
    "          'block3', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n",
    "      resnet_utils.Block(\n",
    "          'block4', bottleneck, [(2048, 512, 1)] * 3)\n",
    "  ]\n",
    "  return resnet_v1(inputs, blocks, num_classes, is_training,\n",
    "                   global_pool=global_pool, output_stride=output_stride,\n",
    "                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n",
    "                   reuse=reuse, scope=scope)\n",
    "resnet_v1_50.default_image_size = resnet_v1.default_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
